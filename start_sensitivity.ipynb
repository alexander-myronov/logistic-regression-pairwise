{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file, make_circles\n",
    "\n",
    "\n",
    "from links import LinksClassifier\n",
    "from logit import LogisticRegressionPairwise, LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid, StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from new_experiment_runner.cacher import CSVCacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafiles_toy = [\n",
    "    r'data/diabetes_scale.libsvm',\n",
    "    r'data/breast-cancer_scale.libsvm',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loader(name):\n",
    "    from sklearn.datasets import load_svmlight_file\n",
    "    from scipy.sparse import issparse\n",
    "    filename = 'data/%s.libsvm' % name\n",
    "    if not name in globals():\n",
    "        X, y = load_svmlight_file(filename)\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "        globals()[name] = (X, y)\n",
    "    return globals()[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = OrderedDict([(os.path.split(f)[-1].replace('.libsvm', ''),\n",
    "                             load_svmlight_file(f))\n",
    "                            for f in datafiles_toy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets['circles'] = make_circles(n_samples=400, noise=0.1, factor=0.51)\n",
    "datasets['moons'] = make_circles(n_samples=400, noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y= datasets.values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_dataset(X, y, percent_labels, percent_links, unlabeled=True, random_state=42):\n",
    "    if random_state:\n",
    "        np.random.seed(random_state)\n",
    "    # np.random.seed(44)\n",
    "    num = int(len(y) * percent_links)\n",
    "\n",
    "    choice1 = next(StratifiedShuffleSplit(n_splits=1, train_size=percent_links).split(X, y))[0]\n",
    "    choice1 = np.in1d(np.arange(len(y)), choice1)\n",
    "    \n",
    "    \n",
    "    choice2 = next(StratifiedShuffleSplit(n_splits=1, train_size=percent_links).split(X, y))[0]\n",
    "    choice2 = np.in1d(np.arange(len(y)), choice2)\n",
    "    \n",
    "    z = (y[choice1] == y[choice2]).astype(float)\n",
    "\n",
    "    \n",
    "    links_index = choice1 | choice2\n",
    "    #print(links_index.sum())\n",
    "    \n",
    "\n",
    "    if percent_labels < 1:\n",
    "        not_links_where = np.where(~links_index)[0]\n",
    "        labels_choice = \\\n",
    "            next(StratifiedShuffleSplit(n_splits=1, \n",
    "                                        train_size=int(percent_labels * len(y))).split(X[not_links_where], y[not_links_where]))[0]\n",
    "        \n",
    "        #print(not_links_where.shape)\n",
    "        labels_choice = not_links_where[labels_choice]\n",
    "    else:\n",
    "        raise Exception()\n",
    "        #labels_choice = np.arange(0, len(X))\n",
    "    labels_index = np.in1d(np.arange(len(y)), labels_choice)\n",
    "    \n",
    "    unsup_index = ~(labels_index & links_index)\n",
    "    \n",
    "    #print(labels_index.sum(), links_index.sum(), unsup_index.sum())\n",
    "    assert (labels_index | links_index | unsup_index).sum() == len(y)\n",
    "    \n",
    "    return X[labels_index], y[labels_index], X[choice1], X[choice2], z, X[unsup_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy_scorer(estimator, X, y):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    y_pred = estimator.predict(X)\n",
    "    y_true = np.copy(y)\n",
    "    y_true[y_true == -1] = 0\n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cacher = CSVCacher(filename='data/start_sensitivity.csv')\n",
    "context = {}\n",
    "for ds_name, (X, y) in datasets.iteritems():\n",
    "    context['dataset'] = ds_name\n",
    "    X_r, y_r, X1, X2, z, Xu = split_dataset(X, y, percent_labels=0.3, percent_links=0.3, unlabeled=True)\n",
    "    continue\n",
    "    for method in ['zeros','normal','normal_univariate','normal_multivariate','random_labels','random_links_diff']:\n",
    "        context['method'] = method\n",
    "        for k in xrange(100):\n",
    "            context['k'] = k\n",
    "\n",
    "            if len(cacher.get(context) > 0):\n",
    "                break\n",
    "\n",
    "            estimator = LinksClassifier(init=method)\n",
    "            grid = {\n",
    "                #'alpha': [0.01, 0.1, 1, 10],\n",
    "                'gamma': [1],#[1e-4, 1e-3, 1e-2, 1e-1, 1],\n",
    "                'kernel': ['rbf'],\n",
    "                #'beta': [0.1, 0.2, 0.3, 0],\n",
    "                #'delta': []\n",
    "            }\n",
    "            full_index = np.ones(len(X_r), dtype=bool)\n",
    "\n",
    "            gs = GridSearchCV(estimator=estimator,\n",
    "                              param_grid=grid,\n",
    "                              cv=[(full_index, full_index)],\n",
    "                              scoring=accuracy_scorer,\n",
    "                              fit_params={\n",
    "                                  'X1': X1,\n",
    "                                  'X2': X2,\n",
    "                                  'z': z,\n",
    "                                  'Xu': Xu\n",
    "                              },\n",
    "                              refit=True,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=True)\n",
    "            gs.fit(X_r, y_r)\n",
    "\n",
    "            last_loss= gs.best_estimator_.last_loss\n",
    "            train_score = accuracy_scorer(gs, X_r, y_r)\n",
    "            cacher.set(context, {'loss': last_loss, 'train_score': train_score})\n",
    "            cacher.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
